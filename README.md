# ETL Pipeline with Python, PostgreSQL, and dbt

A modern, lightweight ETL pipeline that extracts data from a public API, loads it into a PostgreSQL database, and transforms it using dbt. Includes automated data testing.

## ğŸ›  Stack

- **Python** â€” extraction and loading
- **PostgreSQL (Docker)** â€” data warehouse
- **dbt** â€” transformations and testing
- **GitHub** â€” version control

## ğŸ“ Project Structure

    etl_pipeline_project/
    â”œâ”€â”€ etl/ # Python ETL scripts
    â”œâ”€â”€ dbt/etl_dbt/ # dbt project
    â”œâ”€â”€ orchestration/ # (future: Prefect flows)
    â”œâ”€â”€ ci_cd/ # (future: GitHub Actions)
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ README.md


## ğŸš€ How to Run

1. Clone repo and create virtual environment
2. Install dependencies:
```pip install -r requirements.txt```
3. Start PostgreSQL:
```docker run --name etl-postgres -e POSTGRES_PASSWORD=etlpass -p 5433:5432 -d postgres```
4. Run the pipeline:
```python etl/extract_and_load.py```
5. Transform and test with dbt:
```
cd dbt/etl_dbt
dbt run
dbt test
```

---

## âœ… Features

- Pulls fake user data from `randomuser.me`
- Cleans and stages data with dbt
- Validates uniqueness and nulls
- Fully isolated in Docker + venv

---

## ğŸ“Œ Author

**Shaheryar**  
_MSc Data Science & Engineering, Uppsala University_

